+++
title = "一些关于垂直网站爬虫的思考"
date = 2009-09-01

categories = ["Tech"]
tags = ["Crawler"]
+++

**一些关于垂直网站爬虫的思考**
------

基本指标和难点
===
1. 实时性
 - 新的内容，需要很快的抓到
2. 全面性
 - 老的内容和新的内容都需要能抓到，不能只侧重某一方面
 - 新的内容在一定时间后就是老的内容了
 - 基于1，可能新内容要抓取的优先级更高，但不能只抓新的，不要旧的
3. 去除重复
 - 不同的网站可能有相同的内容，抓完后可能需要去掉重复的内容
 - 有写网站的url后面带有随机数，或者无效的参数，但是内容却是固定的，需要想办法去掉，只抓一次
4. 去除循环抓取
 - 各个网站之间可能相互link，需要职能的分析出已经抓取过的
5. 可持续抓取性
 - 一个入口可以一直在抓取，无需人工干预，7*24小时服务
6. 人工干预
 - 抓取速度
 - 抓取进程个数
7. 可能被封
 - 利用6 降低抓取频率
 - 多IP对外抓取
 - 通过Proxy


子系统简单分析
===
1. crawler，专门负责抓取的服务，输入是一个url，输入是一个url对应的页面
2. scheduler，专门负责crawler的调度，能够控制crawler的各方面参数，可能需要多进程共享
3. linkdb, 用来存取link的，非常简单，只有增删改查，注意大数据，要提供对于link抓取优先级的接口，永远提供优先级的topN个link
4. repository, 用来存取content的，非常简单，只有增删改查，注意大数据
5. analyzer, 分析repository的内容，进行去重复，为索引服务


抓取模型
===
1. 多线程
 - 实现简单，python的多线程无法重复利用多核
2. 多进程
 - 实现简单，python的多进程能利用多核
3. 异步
 - 实现复杂，重复利用多核
 - 性能高


